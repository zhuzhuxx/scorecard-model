{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3af15ce0b542bdb804daacd1918cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='Starting Spark application:', max=2400.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SessionID: 4658\n",
      "YARN Application ID: application_1543970679256_0384\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f79eda268d0>\n",
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                            <table  width=80% border=\"2\" align=\"left\">\n",
       "                                <tr>\n",
       "                                    <th style=\"text-align:left\" width=40%>附加函数</th>\n",
       "                                    <th style=\"text-align:left\" width=60%>功能说明</th>\n",
       "                                </tr>\n",
       "                                <tr>\n",
       "                                    <td style=\"text-align:left\" width=40%>save_lab_project(filename)</td>\n",
       "                                    <td style=\"text-align:left\" width=60%>将文件保存到当前Jupyter工程目录中</td>\n",
       "                                </tr>\n",
       "                                <tr>\n",
       "                                    <td style=\"text-align:left\" width=40%>savePersonalFile(filename,overwrite=False)</td>\n",
       "                                    <td style=\"text-align:left\" width=60%>将文件保存到DC“我的资源”中</td>\n",
       "                                </tr>\n",
       "                                 <tr>\n",
       "                                    <td style=\"text-align:left\" width=40%>open(file,mode='r')</td>\n",
       "                                    <td style=\"text-align:left\" width=60%>打开本地或者dc文件</td>\n",
       "                                </tr>\n",
       "                                <tr>\n",
       "                                    <td style=\"text-align:left\" width=40%>load_tlib(spark)</td>\n",
       "                                    <td style=\"text-align:left\" width=60%>加载同盾算法库（默认已加载，算法更新后需重新加载）</td>\n",
       "                                </tr>\n",
       "                            </table>\n",
       "                            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<aspectlib.Rollback object at 0x7f7a68453c58>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import graphviz \n",
    "import pydotplus\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from td.tools import * \n",
    "from itertools import combinations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class split_bins:\n",
    "    def __init__(self,label,feature,data,bins=5,split=[],min_cnt=20,min_rate=0.05,nan=[],type='ks'):\n",
    "        self.label = label\n",
    "        self.feature = feature\n",
    "        self.data = data\n",
    "        self.bins = bins\n",
    "        self.split = set()\n",
    "        self.min_cnt = min_cnt\n",
    "        self.min_rate = min_rate\n",
    "        self.max_iv = -1\n",
    "        self.nan = nan\n",
    "        self.type = type\n",
    "\n",
    "    def cal_main(self,cal_data):\n",
    "        if (len(cal_data)<self.min_cnt) | (float(len(cal_data))/float(len(self.data))<self.min_rate)| (len(self.split)>5): #(np.std(data[self.feature])/np.mean(data[self.feature])<0.15)\n",
    "            return np.nan\n",
    "        else:\n",
    "            k,value = self.cal_ks(cal_data) \n",
    "            if k==1:\n",
    "               return np.nan\n",
    "            else: \n",
    "                cal_data_left = cal_data[cal_data[self.feature]<value]\n",
    "                cal_data_right = cal_data[cal_data[self.feature]>value]  \n",
    "                self.split.update([value])\n",
    "                re_right = self.cal_main(cal_data_left)\n",
    "                re_left = self.cal_main(cal_data_right)\n",
    "    \n",
    "    def cal_ks(self,data):\n",
    "        tmp = data[[self.feature,self.label]].sort_values(self.feature,ascending=False)\n",
    "        one_cnt = sum(tmp[self.label])\n",
    "        zero_cnt = len(tmp) -one_cnt\n",
    "        if (one_cnt==0)|(zero_cnt==0):\n",
    "           return 1,np.nan\n",
    "        else:\n",
    "           tmp['one_cum'] = np.cumsum(tmp[self.label])\n",
    "           tmp['zero_cum'] = np.array([x+1 for x in range(len(tmp))])- np.array(tmp['one_cum'])\n",
    "           tmp['one_cum'] = tmp['one_cum']/one_cnt\n",
    "           tmp['zero_cum'] = tmp['zero_cum']/zero_cnt\n",
    "           tmp['ks_diff'] = map(lambda x,y:abs(x-y),tmp['one_cum'],tmp['zero_cum'])\n",
    "           \n",
    "           ks = max(tmp['ks_diff'])\n",
    "           value = tmp[tmp['ks_diff']==ks][self.feature].iloc[0]\n",
    "           return ks,value\n",
    "\n",
    "    def bin_cal_ks(self):\n",
    "        tmp = self.bin(self.data)\n",
    "        tmp = tmp[['bin_number',self.label]].sort_values('bin_number',ascending=False)\n",
    "        \n",
    "        one_cnt = sum(tmp[self.label])\n",
    "        zero_cnt = len(tmp) -one_cnt\n",
    "        \n",
    "        tmp['one_cum'] = np.cumsum(tmp[self.label])\n",
    "        tmp['zero_cum'] = np.array([x+1 for x in range(len(tmp))])- np.array(tmp['one_cum'])\n",
    "        tmp['one_cum'] = tmp['one_cum']/one_cnt\n",
    "        tmp['zero_cum'] = tmp['zero_cum']/zero_cnt\n",
    "\n",
    "        tmp['ks_diff'] = map(lambda x,y:abs(x-y),tmp['one_cum'],tmp['zero_cum'])\n",
    "        \n",
    "        ks = max(tmp['ks_diff'])\n",
    "        value = tmp[tmp['ks_diff']==ks]['bin_number'].iloc[0]\n",
    "        return ks,value\n",
    "\n",
    "\n",
    "    def bin(self,data):\n",
    "        tmp = data \n",
    "        quan = [round(float(x)/100*len(tmp),0) for x in range(0,100,5)]\n",
    "        bin_number=0\n",
    "        bin_split = []\n",
    "        for i in range(len(quan)-1):\n",
    "           bin_number = bin_number +1\n",
    "           bin_split.extend([bin_number]*int(quan[i+1]-quan[i]))\n",
    "        bin_split.extend([bin_number+1]*int(len(tmp)-quan[i+1]))\n",
    "        tmp['bin_number'] = bin_split\n",
    "        return tmp\n",
    "    \n",
    " \n",
    "    def cal_iv(self,cal_data,is_simple,split):\n",
    "        if len(split)>0:\n",
    "           split = sorted(list(split),reverse=False)\n",
    "           one_cnt = sum(cal_data[self.label])\n",
    "           zero_cnt = len(cal_data) - one_cnt\n",
    "           split = sorted(split)\n",
    "           cal_data['bin'] = map(lambda x:self.bin_name(x,split),cal_data[self.feature])\n",
    "           piv_table = pd.pivot_table(cal_data,index='bin',values=[self.label],aggfunc=[np.sum,len])\n",
    "           \n",
    "           tmp = pd.DataFrame({'bin':list(piv_table.index),'one':list(piv_table['sum'][self.label]),'total':list(piv_table['len'][self.label])})\n",
    "           tmp['zero'] = tmp['total']-tmp['one']\n",
    "           tmp['one_ratio'] = tmp['one']/one_cnt\n",
    "           tmp['zero_ratio'] = tmp['zero']/zero_cnt\n",
    "           smoothness = min(np.percentile(tmp['one_ratio'],5),np.percentile(tmp['zero_ratio'],5))/100\n",
    "           tmp['woe'] = map(lambda x,y:-np.log((x+smoothness)/(y+smoothness)),tmp['one_ratio'],tmp['zero_ratio'])\n",
    "           tmp['iv_sep'] = map(lambda x,y:np.log((x+smoothness)/(y+smoothness))*(x-y),tmp['one_ratio'],tmp['zero_ratio'])\n",
    "           tmp['iv'] = [sum(tmp['iv_sep'])]*len(tmp)\n",
    "           tmp['bad_rate'] =  tmp['one']/tmp['total']\n",
    "           tmp['left_bin'] = [x[1:-1].split(',')[0] for x in tmp['bin']]\n",
    "           tmp['right_bin'] = [x[1:-1].split(',')[1] for x in tmp['bin']]\n",
    "            \n",
    "           tmp['left_bin'] =[float(x) if 'inf' not in x else -np.inf for x in tmp['left_bin']]\n",
    "           tmp = tmp.sort_values(by='left_bin',ascending=True)\n",
    "           if is_simple==True:\n",
    "               return tmp['iv'].iloc[0]\n",
    "           else:\n",
    "               return tmp[['bin','one','zero','total','bad_rate','woe','iv_sep','iv','left_bin','right_bin']]\n",
    "        else:\n",
    "           one_cnt = sum(cal_data[self.label])\n",
    "           zero_cnt = len(cal_data) - one_cnt\n",
    "           return pd.DataFrame({'bin':'NA','one':[one_cnt],'zero':[zero_cnt],'total':[len(cal_data)],'bad_rate':[float(one_cnt)/len(cal_data)],'woe':[np.nan],'iv_sep':[np.nan],'iv':[np.nan]})\n",
    "\n",
    "    def bin_name(self,x,split):\n",
    "        split_dev = [1 if x>=i else 0 for i in split]\n",
    "        try:\n",
    "          flag = split_dev.index(0)\n",
    "        except:\n",
    "          return '['+str(split[len(split)-1])+',inf)'\n",
    "        \n",
    "        if flag==0:\n",
    "           return '(-inf,'+str(split[flag])+')' \n",
    "        else:\n",
    "           return '['+str(split[flag-1])+','+str(split[flag])+')'\n",
    "    \n",
    "    def combine_bin(self,reduce_bins):\n",
    "        combine_list = list(combinations(self.split,reduce_bins-1))\n",
    "        res = pd.DataFrame()\n",
    "        for sp in combine_list:\n",
    "            sp = sorted(sp)\n",
    "            sp_set = '_'.join([str(x) for x in sp])\n",
    "            res = res.append(pd.DataFrame({'iv':ss.cal_iv(cal_data=ss.data,is_simple=True,split=sp),'split':[sp_set]}))\n",
    "        res = res.reset_index(drop = True)\n",
    "        new_split =  res.sort_values('iv',ascending=False)\n",
    "        if len(new_split)>0:\n",
    "            \n",
    "            for l in range(len(new_split)):\n",
    "                flag = 1\n",
    "                s =[float(x) for x in new_split['split'].iloc[l].split('_')]\n",
    "                for i in range(len(s)):\n",
    "                    if i==0:\n",
    "                       if sum(data[self.feature]<s[i])<self.min_cnt:\n",
    "                          flag = 0\n",
    "                          break \n",
    "                    elif i<(len(s)-1):\n",
    "                       if sum((data[self.feature]>=s[i-1])&(data[self.feature]<s[i]))<self.min_cnt:\n",
    "                          flag = 0\n",
    "                          break \n",
    "                    else:\n",
    "                       if sum(data[self.feature]>=s[i-1])<self.min_cnt:\n",
    "                          flag = 0\n",
    "                          break \n",
    "                if flag == 1:                                             \n",
    "                       self.max_iv = new_split['iv'].iloc[l]\n",
    "                       self.split = set([float(x) for x in new_split['split'].iloc[l].split('_')])\n",
    "                       return 1 \n",
    "            return 0\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def combine_main(self):\n",
    "        combine_list = sorted(range(2,min(self.bins,len(self.split)+1)+1),reverse=True)\n",
    "        for combine_list_i in combine_list:\n",
    "            res = self.combine_bin(combine_list_i)\n",
    "            if res==1:\n",
    "               break\n",
    "        return None\n",
    "    \n",
    "    def cal_bin_tree(self,data):\n",
    "        parameters = {'min_samples_split':range(10,50,10),'max_depth':[3],'min_samples_leaf':[self.min_cnt]}\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        gs = GridSearchCV(clf, parameters, cv=5,scoring='roc_auc')\n",
    "        gs.fit(X=data[[self.feature]],y=data[self.label]) \n",
    "        node = gs.best_estimator_.apply(data[[self.feature]])\n",
    "        ''' \n",
    "        tree = DecisionTreeClassifier(min_samples_split=20,min_samples_leaf=self.min_cnt,max_depth=3)\n",
    "        tree.fit(X=data[[self.feature]],y=data[self.label])\n",
    "        node = tree.apply(data[[self.feature]])\n",
    "        '''\n",
    "        \n",
    "        tmp = pd.DataFrame({self.feature:data[self.feature],'y':data[self.label],'node':node})\n",
    "        node_range = tmp[['node',self.feature]].groupby('node')[self.feature].agg(['min','max'])  \n",
    "        \n",
    "        self.split = set(node_range['min'].values)\n",
    "        self.split.update(set(node_range['max'].values))\n",
    "        #self.split = self.split - set([min(self.split)])\n",
    "        return None\n",
    "  \n",
    "    \n",
    "    def cal_bin_main(self):\n",
    "        data_common = self.data[~self.data[self.feature].isin(self.nan)]\n",
    "        data_uncommon = self.data[self.data[self.feature].isin(self.nan)]  \n",
    "        if self.type == 'ks' : \n",
    "           self.cal_main(data_common)\n",
    "        elif self.type =='tree':\n",
    "           self.cal_bin_tree(data_common)\n",
    "        self.max_iv = self.cal_iv(data_common,is_simple=True,split=self.split)\n",
    "        self.combine_main()\n",
    "        iv_common = self.cal_iv(data_common,is_simple=False,split=self.split) \n",
    "        \n",
    "        if len(data_uncommon)>0:\n",
    "            iv_uncommon = self.cal_iv(data_uncommon,is_simple=False,split=[]) \n",
    "            return iv_common.append(iv_uncommon)\n",
    "        else:\n",
    "            return iv_common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据ETL\n",
    "\n",
    "f = open('/data_d_offline.csv')\n",
    "#dtypes = recommend_dtypes('data_d_offline.csv')\n",
    "data_d_offline = pd.read_csv(f,index_col = 0)#,dtype = dtypes)\n",
    "\n",
    "\n",
    "#标签修正\n",
    "data_d_offline['label_org'] = data_d_offline['label']\n",
    "data_d_offline['label'] = [ 1 if x=='Bad' else 0 if x=='Good' else 2 for x in data_d_offline['label_org']]\n",
    "\n",
    "\n",
    "#\n",
    "dim = ['m','i']\n",
    "d_range = ['1day','7day','15day','30day','90day','180day','360day']\n",
    "\n",
    "for dim_i in dim:\n",
    "    for d_range_i  in d_range:\n",
    "\n",
    "        dim_freq_record_Loan_others = dim_i+'_freq_record_Loan'+'_others_'+d_range_i\n",
    "        dim_cnt_partner_Loan_others = dim_i+'_cnt_partner_Loan'+'_others_'+d_range_i\n",
    "        \n",
    "        data_d_offline[dim_freq_record_Loan_others] = data_d_offline[dim_i+'_freq_record_Loan_all_'+d_range_i]\n",
    "        -data_d_offline[dim_i+'_freq_record_Loan_Imbank_'+d_range_i]-data_d_offline[dim_i+'_freq_record_Loan_P2pweb_'+d_range_i]\n",
    "        -data_d_offline[dim_i+'_freq_record_Loan_Offloan_'+d_range_i]-data_d_offline[dim_i+'_freq_record_Loan_Consumerfinance_'+d_range_i]\n",
    "        -data_d_offline[dim_i+'_freq_record_Loan_Bank_'+d_range_i]\n",
    "\n",
    "        data_d_offline[dim_cnt_partner_Loan_others] = data_d_offline[dim_i+'_cnt_partner_Loan_all_'+d_range_i]\n",
    "        -data_d_offline[dim_i+'_cnt_partner_Loan_Imbank_'+d_range_i]-data_d_offline[dim_i+'_cnt_partner_Loan_P2pweb_'+d_range_i]\n",
    "        -data_d_offline[dim_i+'_cnt_partner_Loan_Offloan_'+d_range_i]-data_d_offline[dim_i+'_cnt_partner_Loan_Consumerfinance_'+d_range_i]\n",
    "        -data_d_offline[dim_i+'_cnt_partner_Loan_Bank_'+d_range_i]\n",
    "\n",
    "        #data_d_offline = data_d_offline.drop([dim_i+'_freq_record_Loan_all_'+d_range_i,dim_i+'_cnt_partner_Loan_all_'+d_range_i],axis=1)\n",
    "\n",
    "data_d_offline_part = data_d_offline[(data_d_offline['label']==0)|(data_d_offline['label']==1)]\n",
    "data_d_offline_part['t1_result'] = data_d_offline_part['t1_result'].astype(str)\n",
    "data_d_offline_part['t1_result']=[float(x) if x in ['0','1','2','3','4','5','6','7','8','9','10','11','12','13'] else -999 for x in data_d_offline_part['t1_result']]  \n",
    "\n",
    "\n",
    "data_d_offline_nan = data_d_offline_part.replace(-999,np.nan)\n",
    "data_d_offline_part0 = data_d_offline_nan.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cal_single_rate(dx,nan_list):\n",
    "    '''\n",
    "    非null单一值占比\n",
    "    '''\n",
    "    max_single_cnt = dx[~dx.isin(nan_list)].value_counts().sort_values(ascending=False).iloc[0]\n",
    "    return float(max_single_cnt)/(sum(~dx.isin(nan_list))+0.001)\n",
    "\n",
    "\n",
    "def get_uncor(df,info_select,feature_name,sort_factor,r_value=0.8):\n",
    "    '''\n",
    "    相关性\n",
    "    '''\n",
    "    save_feature = set()\n",
    "    cut_feature = set() \n",
    "    start_feature= set(df.columns)\n",
    "    while len(start_feature)>0:  \n",
    "              x = start_feature.pop() \n",
    "              index = df[df[x]>r_value][x].index \n",
    "              index = set(index) & start_feature \n",
    "              if True:\n",
    "                    index.update(set([x])) \n",
    "                    cal_index = list(index) \n",
    "                    save_feature.update([info_select[info_select[feature_name].isin(cal_index)].sort_values(sort_factor,ascending=False)[feature_name].iloc[0]])\n",
    "                    cut_feature.update(set(cal_index))\n",
    "                    start_feature = start_feature - set(cal_index) \n",
    "\n",
    "    return save_feature\n",
    "\n",
    "\n",
    "def cal_corr(data,info_select,feature_name,sort_factor,r_value=0.8):\n",
    "    '''\n",
    "    通过特征相关性筛选\n",
    "    '''\n",
    "    data_dtypes = data.dtypes\n",
    "    obj_col = [x for x in data.columns if data_dtypes[x]=='O']\n",
    "    nor_col = [x for x in data.columns if data_dtypes[x]!='O']\n",
    "    all_feature_key = ['_'.join(x.split('_')[0:-1]) for x in nor_col]\n",
    "    \n",
    "    data = data.replace(-999,np.nan)\n",
    "    data = data.replace(-1111,np.nan)\n",
    "    #同计算逻辑指标剔除相关性较高\n",
    "    select = set()\n",
    "    for x in set(all_feature_key): \n",
    "        if ('accept' in x.lower())|('reject' in x.lower())|('review' in x.lower()):\n",
    "            continue\n",
    "        else:\n",
    "            col_x = list(info_select[info_select['feature_key']==x][feature_name])  \n",
    "            tmp = data[list(set(col_x))] \n",
    "            cor = tmp.corr(method='spearman')\n",
    "            r1 = get_uncor(cor,info_select,feature_name,sort_factor,r_value)\n",
    "            select.update(set(r1))\n",
    "\n",
    "            \n",
    "    #different key\n",
    "    select_diff = set()\n",
    "    tmp = data[list(select)] \n",
    "    cor = tmp.corr(method='spearman')\n",
    "    select_diff = get_uncor(cor,info_select,feature_name,sort_factor,r_value)\n",
    "    return list(select_diff)+obj_col\n",
    "\n",
    "\n",
    "def cor_label(x,y):\n",
    "    if x.dtype!='O':\n",
    "        data=pd.DataFrame({'x':x.values,'y':y.values})\n",
    "        data = data.dropna()\n",
    "        return abs(data.corr(method='spearman').iloc[0,1])\n",
    "    else:\n",
    "        return 999\n",
    "\n",
    "def data_pre_processing(data,col_name,label_name,cal_all=0,single_rate=0.5,r_value=0.8,nan_list=[-999]):\n",
    "    \n",
    "    if cal_all==0:\n",
    "        data_single_rate = data[col_name].apply(lambda x :cal_single_rate(x,nan_list),axis=0)\n",
    "        data_single = data[data_single_rate[data_single_rate<=single_rate].index] \n",
    "        info_select = data_single.apply(lambda x:cor_label(x,y=data[label_name]))\n",
    "        info_select = info_select.to_frame()\n",
    "        info_select.columns=['rate']\n",
    "        info_select['feature_name'] = info_select.index\n",
    "        info_select['feature_key'] = ['_'.join(x.split('_')[0:-1]) for x in info_select['feature_name']]\n",
    "        \n",
    "        data_cor_col = cal_corr(data_single,info_select,'feature_name','rate',r_value)\n",
    "        data_cor = data_single[data_cor_col] \n",
    "        \n",
    "        return data_cor\n",
    "    else: \n",
    "        data_single_rate = data[col_name].apply(lambda x :cal_single_rate(x,nan_list),axis=0) \n",
    "        info_select = data[col_name].apply(lambda x:cor_label(x,y=data[label_name]))\n",
    "        \n",
    "        result = pd.concat([data_single_rate,info_select],axis=1)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "def cal_bin(data,factor,feature,bad_name='bad', good_name='good', piece=5, rate=0.005, min_bin_size=50, not_in_list=[-1111,-999,np.nan]):\n",
    "    \n",
    "    ss=split_bins(label=factor,feature,data=data,bins=piece,split=[],min_cnt=min_bin_size,min_rate=rate,nan=not_in_list,type='ks')\n",
    "    tmp = ss.cal_bin_main()\n",
    " \n",
    "    \n",
    "    if len(tmp)>0:\n",
    "       tmp['iv_total'] = [sum(tmp['IV'])]*len(tmp)\n",
    "       tmp['feature_name'] = [feature]*len(tmp) \n",
    "       tmp['bad_cover_rate'] = tmp['bad_rate']*tmp['total_count']\n",
    "       tmp['bad_cover_rate'] =  tmp['bad_cover_rate'].cumsum()\n",
    "       tmp['bad_cover_rate'] = tmp['bad_cover_rate']*1.0/max(tmp['bad_cover_rate'])\n",
    "    \n",
    "       tmp['good_cover_rate'] = (1-tmp['bad_rate'])*tmp['total_count']\n",
    "       tmp['good_cover_rate'] =  tmp['good_cover_rate'].cumsum()\n",
    "       tmp['good_cover_rate'] = tmp['good_cover_rate']*1.0/max(tmp['good_cover_rate'])    \n",
    "    else:\n",
    "       tmp=pd.DataFrame({'feature_name':[feature]})\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def cal_iv(data,factor):\n",
    "    data_bin = map(lambda x:cal_bin(data,factor,feature=x,bad_name='bad', good_name='good', piece=5, rate=0.005, min_bin_size=50, not_in_list=[-1111,-999,np.nan])\n",
    "                  ,data.columns)\n",
    "    return data_bin\n",
    "\n",
    "\n",
    "def woe_re(x,woe_info,nan_list):\n",
    "    if x in nan_list:\n",
    "        return woe_info[woe_info['Bin']==\"NA\"]['WOE'].iloc[0]\n",
    "    for i in range(len(woe_info)):\n",
    "        if (x>woe_info['left_bin'].iloc[i])&(x<=woe_info['right_bin'].iloc[i]):\n",
    "            return woe_info['WOE'].iloc[i]\n",
    "    return 'error in find bin!!!'\n",
    "    \n",
    "    \n",
    "def woe_replacement(data,woe_info,col_name,nan_list):\n",
    "    data_woe=copy.deepcopy(data)\n",
    "    for col_i in col_name:\n",
    "        data_woe[col_i]=map(lambda x:woe_re(x,woe_info[woe_info['feature_name']==col_i],nan_list),data_woe[col_i])\n",
    "    return data_woe[col_name]\n",
    "        \n",
    "\n",
    "def select_iv(input_col,iv_info,alpha):\n",
    "    output_col=[]\n",
    "    for x in input_col:\n",
    "        if iv_info[iv_info['feature_name']==x]['iv_total'].iloc[0]>=alpha:\n",
    "            output_col.append(x)\n",
    "    return output_col\n",
    "    \n",
    "\n",
    "def cal_model(data,col_name,label_data):\n",
    "    lr_result = pd.DataFrame()\n",
    "    #Lasso 选入模变量\n",
    "    for x in [0.001, 0.01, 0.1, 1]:\n",
    "        tmp_dict = dict()\n",
    "        parameters = {'C': [x]}  \n",
    "        clf = LogisticRegression(random_state=0, penalty='l1')\n",
    "        clf_gs =  GridSearchCV(clf, parameters, cv=5,scoring='roc_auc')\n",
    "        clf_gs.fit(data_woe_done[sec_select_col],label_data)\n",
    "      \n",
    "        lr = LogisticRegression(random_state=0, penalty='l1',C=x)\n",
    "        lr.fit(data[col_name],label_data)\n",
    "    \n",
    "        for k,v in zip(sec_select_col,lr.coef_[0]):\n",
    "            tmp_dict[k]=v \n",
    "        tmp_dict['intercept'] = lr.intercept_[0]\n",
    "        tmp_dict['paramters']=x\n",
    "        tmp_dict['mean_test_score'] = clf_gs.cv_results_['mean_test_score']\n",
    "    \n",
    "        tmp_dict_df = pd.DataFrame.from_dict(tmp_dict)\n",
    "        if sum(tmp_dict_df[col_name].iloc[0]<=0)==len(col_name):\n",
    "            tmp_dict_df['symbol_same_dir'] = [1]\n",
    "        else:\n",
    "            tmp_dict_df['symbol_same_dir'] = [0]\n",
    "        lr_result = lr_result.append(tmp_dict_df)\n",
    "    return lr_result \n",
    "\n",
    "\n",
    "\n",
    "def score_card(bin_info,beta ,score_base=600,pdo=20,odds_base=50):\n",
    "    '''\n",
    "    评分卡变量分数\n",
    "    '''\n",
    "    n = len(beta.index)-1\n",
    "    B = round(pdo/np.log(2),2)\n",
    "    A = round(score_base - B*np.log(odds_base),2)\n",
    "    intercept = beta['intercept'] if 'intercept' in beta.index else 0\n",
    "    beta = pd.DataFrame(beta)\n",
    "    beta.columns=['beta']\n",
    "    beta['feature_name']=beta.index.values\n",
    "    bin_info_score = bin_info.merge(beta,left_on='feature_name',right_on='feature_name',how='inner')\n",
    "    bin_info_score['score'] = A/n-B*(bin_info_score['WOE']*bin_info_score['beta'] + intercept/n)\n",
    "    bin_info_score['score'] = [round(x,2) for x in bin_info_score['score']]\n",
    "    return bin_info_score\n",
    "\n",
    "\n",
    "def cal_score(data,beta ,score_base=600,pdo=20,odds_base=50):\n",
    "    '''\n",
    "    分数\n",
    "    '''\n",
    "    n = len(beta.index)-1\n",
    "    B = round(pdo/np.log(2),2)\n",
    "    A = round(score_base - B*np.log(odds_base),2)\n",
    "    intercept = beta['intercept'] if 'intercept' in beta.index else 0\n",
    "    score=pd.DataFrame({'score':[intercept]*len(data)})\n",
    "    for x in beta.index:\n",
    "        score['score'] = [score['score'].iloc[i]+data[x].iloc[i]*beta[x] for i in range(len(score))]\n",
    "    \n",
    "    score['score'] = A-B*score['score']\n",
    "    return score\n",
    "\n",
    "def cal_vif(data,col_name):\n",
    "    '''\n",
    "    vif\n",
    "    '''\n",
    "    flag=True\n",
    "    left_col = set(col_name) \n",
    "    while flag:  \n",
    "        vif_value = []\n",
    "        data_value = data[list(left_col)].values\n",
    "        for i in range(len(left_col)):\n",
    "            v = variance_inflation_factor(data_value,i) \n",
    "            vif_value.append(v)\n",
    "        result = pd.DataFrame({'factor':list(left_col),'vif':vif_value})\n",
    "        result_beta = result[result['vif']>=5].sort_values(by='vif',ascending=False)\n",
    "        if (len(result_beta)== 0):\n",
    "            flag = False\n",
    "        else:\n",
    "            left_col = left_col -set([result_beta['factor'].iloc[0]]) \n",
    "            if len(left_col)==1:\n",
    "                flag = False\n",
    "    return list(left_col)\n",
    "\n",
    "def draw_pr_curve(data,feature,label,beta=0.5):\n",
    "    if len(set(data[label]))!=2:\n",
    "        print 'there are not 2 labels in data'\n",
    "        return None\n",
    "    data[feature] = data[feature].astype(float)\n",
    "    tmp_data = data[[feature,label]].sort_values(feature,ascending=False)\n",
    "    min_value = min(tmp_data[feature])\n",
    "    max_value = max(tmp_data[feature])\n",
    "    \n",
    "    recall = []\n",
    "    precision = [] \n",
    "    f1_score = []\n",
    "    ngrids = 100\n",
    "    I_val = []\n",
    "    ks = []\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    for i in range(ngrids):\n",
    "        i_value = max_value-i*(max_value-min_value)/(ngrids-1)\n",
    "        I_val.append(i_value)\n",
    "        P = len(tmp_data[tmp_data[label]==1])\n",
    "        N = len(tmp_data[tmp_data[label]==0])\n",
    "        TP = len(tmp_data[(tmp_data[feature]>=i_value) & (tmp_data[label]==1)])\n",
    "        FP = len(tmp_data[(tmp_data[feature]>=i_value) & (tmp_data[label]==0)])\n",
    "        \n",
    "        recall.append(TP*1.0/P) \n",
    "        precision.append(TP*1.0/(TP+FP)) \n",
    "        tpr.append(TP*1.0/P)\n",
    "        fpr.append(FP*1.0/N)\n",
    "    f1_score = [(1+beta**2)*recall[i]*precision[i]*1.0/(recall[i]+(1+beta**2)*precision[i]+0.00001) for i in range(len(I_val))]\n",
    "    ks = [abs(tpr[i]-fpr[i]) for  i in  range(len(I_val))]\n",
    "    bestks_index = np.argmax(ks)\n",
    "    bestks = ks[bestks_index]\n",
    "    \n",
    "    best_f1_index = np.argmax(f1_score)\n",
    "    best_f1 = f1_score[best_f1_index]\n",
    "    \n",
    "    plt.figure(figsize=(18,10))\n",
    "    #pr\n",
    "    except_0 = np.where(np.array(recall)>0)[0].tolist()[0]\n",
    "    pr_value = [abs(recall[x]-precision[x]) for x in range(except_0,len(recall))]\n",
    "    balance_index = np.argmin(pr_value)+except_0\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('pr curve')\n",
    "    plt.plot(recall, precision, 'k',recall, precision, 'o') \n",
    "    #plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')   \n",
    "    plt.legend(['best_ks:('+str(round(bestks,4))+','+str(I_val[bestks_index])+')\\n'+'f1_max:('+str(round(best_f1,4))+','+str(I_val[best_f1_index])+')'],loc='lower right')\n",
    "    plt.ylabel('precision', fontsize=12)\n",
    "    plt.xlabel('recall', fontsize=12)\n",
    "    plt.xticks(rotation='vertical') \n",
    "    \n",
    "    pr_df = pd.DataFrame({'ngrids':I_val,'recall':recall,'precision':precision,'f1':f1_score,'ks':ks\n",
    "                         ,'tpr':tpr})\n",
    "    #roc\n",
    "    fpr1,tpr1,threshold = roc_curve(tmp_data[label],tmp_data[feature])\n",
    "    roc_auc = auc(fpr1,tpr1)\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('roc curve')\n",
    "    plt.plot(fpr, tpr, 'k',fpr, tpr,'o') \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  \n",
    "    plt.legend( [ 'ROC curve (area = %0.2f)' % roc_auc],loc=\"lower right\")\n",
    "    \n",
    "    plt.ylabel('tpr', fontsize=12)\n",
    "    plt.xlabel('fpr', fontsize=12)\n",
    "    plt.xticks(rotation='vertical') \n",
    "    \n",
    "    \n",
    "    #ks \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title('ks curve')\n",
    "    plt.plot(range(len(fpr)),fpr,color='blue')  \n",
    "    plt.plot(range(len(tpr)),tpr,color='blue') \n",
    "    plt.plot(range(len(ks)),ks,color='green')\n",
    "    plt.legend(['fpr','tpr','ks:'+str(bestks)])\n",
    "     \n",
    "    plt.subplot(2,2,4)\n",
    "    p90 = np.percentile(data[feature],95)\n",
    "    p10 = np.percentile(data[feature],5)\n",
    "    sns.kdeplot(data[(data[label]==1)&(data[feature]<=p90)&(data[feature]>=p10)][feature], shade=True)\n",
    "    sns.kdeplot(data[(data[label]==0)&(data[feature]<=p90)&(data[feature]>=p10)][feature], shade=True,color='m')\n",
    "    plt.legend(['label=1','label=0'])\n",
    "    \n",
    "    return pr_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_d_offline_part, data_d_offline_part['label'], test_size=0.33, random_state=42)\n",
    "data = X_train\n",
    "\n",
    "day_range = ['1day','7day','15day','30day','90day','180day','360day']\n",
    "col_name = [x for x in data.columns if ('day' in x)|('7day' in x)|('15day' in x)]+['t1_result']\n",
    "\n",
    "data_proc_done = data_pre_processing(data,col_name,'label',cal_all=0,single_rate=0.5,r_value=0.99,nan_list=[-999])\n",
    "\n",
    "data_bin = map(lambda x:cal_bin(data,factor='label',feature=x,bad_name='bad', good_name='good', piece=5, rate=0.005, min_bin_size=50, not_in_list=[-1111,-999,np.nan])\n",
    "                  ,data_proc_done.columns)\n",
    "\n",
    "data_bin_combine = pd.concat(data_bin)\n",
    "\n",
    "data_bin_combine['left_bin'] = data_bin_combine['left_bin'].astype(float)\n",
    "data_bin_combine['right_bin'] = data_bin_combine['right_bin'].astype(float)\n",
    "\n",
    "data_bin_combine['feature_key'] = ['_'.join(x.split('_')[0:-1]) for x in data_bin_combine['feature_name']]\n",
    "\n",
    "#特征二次筛选-iv\n",
    "\n",
    "alpha_80 = data_bin_combine['iv_total'].quantile(q=0.8)\n",
    "alpha_fix = 0.1\n",
    "iv_select_col = select_iv(data_proc_done.columns,iv_info=data_bin_combine,alpha=alpha_fix)\n",
    "\n",
    "data_woe_done = woe_replacement(data=data_proc_done,woe_info=data_bin_combine,col_name=iv_select_col,nan_list=nan_list)\n",
    "\n",
    "\n",
    "#特征二次筛选-相关性\n",
    "data_cor = data_woe_done.corr(method='spearman')\n",
    "sec_select_col = cal_corr(data = data_cor,info_select = data_bin_combine ,feature_name = 'feature_name'\n",
    "                          ,sort_factor = 'iv_total',r_value=0.6)\n",
    "\n",
    "#vif去除多重共线性\n",
    "sec_select_col_vif = cal_vif(data = data_woe_done,col_name = sec_select_col)\n",
    "\n",
    "\n",
    "#Lasso 选入模变量 \n",
    "param_result = cal_model(data = data_woe_done,col_name=sec_select_col_vif,label_data=y_train)\n",
    "if sum(param_result['symbol_same_dir']>0)>0:\n",
    "    score_card_beta = param_result[param_result['symbol_same_dir']>0].sort_values(by=['symbol_same_dir','mean_test_score'],ascending=(False,False))[sec_select_col].iloc[0]\n",
    "    score_card_beta = score_card_beta[score_card_beta!=0]\n",
    "    \n",
    "else:\n",
    "    print('symbol direction is not same')\n",
    "    \n",
    "\n",
    "#评分卡\n",
    "scorecard_model = score_card(bin_info=data_bin_combine,beta = score_card_beta,score_base=600,pdo=20,odds_base=50)\n",
    "\n",
    "\n",
    "#评分卡分数\n",
    "scorecard_result = cal_score(data=data_woe_done,beta=score_card_beta ,score_base=600,pdo=20,odds_base=50)\n",
    "\n",
    "\n",
    "#绘制评估曲线\n",
    "plt.switch_backend('agg')\n",
    "data =pd.DataFrame({'label':y_train.values,'score':max(scorecard_result['score'])-scorecard_result['score']}) \n",
    "pr_df = draw_pr_curve(data = data,feature = 'score' ,label = 'label') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss=split_bins(label='label',feature='i_cnt_partner_Loan_all_360day',data=data,bins=5,split=[],min_cnt=20,min_rate=0.05,nan=[-999],type='ks')\n",
    "ss.cal_bin_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss=split_bins(label='label',feature='i_cnt_partner_Loan_all_360day',data=data,bins=5,split=[],min_cnt=20,min_rate=0.05,nan=[-999],type='tree')\n",
    "ss.cal_bin_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
